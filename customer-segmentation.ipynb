{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-07-31T13:52:20.572986Z","iopub.status.busy":"2024-07-31T13:52:20.572539Z","iopub.status.idle":"2024-07-31T13:52:20.586325Z","shell.execute_reply":"2024-07-31T13:52:20.585037Z","shell.execute_reply.started":"2024-07-31T13:52:20.572934Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Links:\n","    - Kaggle Dataset: https://www.kaggle.com/datasets/vetrirah/customer\n","    - github: https://github.com/andycrossman11/Customer_Segmentation_Project"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:05:06.713890Z","iopub.status.busy":"2024-07-31T15:05:06.713497Z","iopub.status.idle":"2024-07-31T15:05:06.719198Z","shell.execute_reply":"2024-07-31T15:05:06.717948Z","shell.execute_reply.started":"2024-07-31T15:05:06.713861Z"},"trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","    - A marketing client has customer related data and has hypothesized that there are 4 types of customers.\n","    - The data provided has a 'Segmentation' assignment of 'A', 'B', 'C', or 'D' for each customer.\n","    - The client has asked to provide a clustering model that validates this segmentation hypothesis.\n","    \n","    - Therefore, as a validation task, this work calls for an unsupervised learning algorithm.\n","    - If I were to utilize the client's hypothesis as ground truth labels in a supervised learning algorithm, the objective function of the alg. will be optimized to make the hypothesis look correct. Such a bias is incorrect and therefore in order to validate the assignment, I would need to find a vector space that confirms the hypothesis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:00.612905Z","iopub.status.busy":"2024-07-31T15:41:00.612233Z","iopub.status.idle":"2024-07-31T15:41:00.660547Z","shell.execute_reply":"2024-07-31T15:41:00.659416Z","shell.execute_reply.started":"2024-07-31T15:41:00.612868Z"},"trusted":true},"outputs":[],"source":["train_data = pd.read_csv(\"/kaggle/input/customer/Train.csv\")\n","\n","print(train_data.head(5))"]},{"cell_type":"markdown","metadata":{},"source":["## Perform initial EDA:\n","    - Look at each feature(datatype, distrib, histogram, etc).\n","    - Look for missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:02.289196Z","iopub.status.busy":"2024-07-31T15:41:02.288614Z","iopub.status.idle":"2024-07-31T15:41:02.356016Z","shell.execute_reply":"2024-07-31T15:41:02.354975Z","shell.execute_reply.started":"2024-07-31T15:41:02.289152Z"},"trusted":true},"outputs":[],"source":["print(train_data.info())\n","\n","print(train_data.describe(include=\"all\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:03.008603Z","iopub.status.busy":"2024-07-31T15:41:03.008186Z","iopub.status.idle":"2024-07-31T15:41:04.839102Z","shell.execute_reply":"2024-07-31T15:41:04.837660Z","shell.execute_reply.started":"2024-07-31T15:41:03.008571Z"},"trusted":true},"outputs":[],"source":["for column in train_data.select_dtypes(include=['object']).columns:\n","    train_data[column].value_counts().plot(kind='bar')\n","    plt.title(f'Histogram for {column}')\n","    plt.xlabel(column)\n","    plt.ylabel('Frequency')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Categorical Variable Analysis:\n","    - Gender Skewed towards Male\n","    - Ever_Married skewed towards Yes\n","    - Graduated skewed towards Yes\n","    - Profession has skew as Artist and Healthcare make up roughly 1/2 of all data\n","    - Spending Score has skew with over 1/2 of all data being 'Low'\n","    - Segmentation is roughly uniform across the 4 customer types defined by the marketing agency"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:04.892290Z","iopub.status.busy":"2024-07-31T15:41:04.891408Z","iopub.status.idle":"2024-07-31T15:41:05.886193Z","shell.execute_reply":"2024-07-31T15:41:05.885029Z","shell.execute_reply.started":"2024-07-31T15:41:04.892250Z"},"trusted":true},"outputs":[],"source":["for column in train_data.select_dtypes(include=['int64', 'float64']).columns:\n","    if column != \"ID\":\n","        train_data[column].hist(bins=30)\n","        plt.title(f'Histogram for {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Frequency')\n","        plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## Numeric Variable Analysis:\n","    - All 3 appear to be from gamma distributions as the majority of data is on the left of center with a right skew\n","    - From the histogram, it looks like Family_Size could actually be treated as categorical. This would be a mistake as switching to a categorical representation would lose generality with unseen data. Family size could theoretically be an positive integer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:06.559820Z","iopub.status.busy":"2024-07-31T15:41:06.559399Z","iopub.status.idle":"2024-07-31T15:41:06.575654Z","shell.execute_reply":"2024-07-31T15:41:06.574348Z","shell.execute_reply.started":"2024-07-31T15:41:06.559787Z"},"trusted":true},"outputs":[],"source":["missing_percentages = train_data.isnull().sum() * 100 / len(train_data)\n","print(missing_percentages)"]},{"cell_type":"markdown","metadata":{},"source":["## Handling Missing Values\n","    - Ever_Married is missing ~ 1.74% of values. The skew is fairly small, and certainly not large enough to just convert missing values to 'yes'. To handle the missing values I will replace nan with 'Missing' thus turning the binary categorical var to a 3-way categorical var.\n","    - Graduated will be treated in the same manner since the skew is not that high\n","    - Since Profession has so many types already, it makes sense to add a 'missing' type to handle the missing data rather than assigning to 'artist' type\n","       - Var_1 is is so heavily skewed as a histogram that intuitively it makes sense to impute with the high frequency type, 'Cat_6'.\n","    - Work_Experience looks to follow a gamma distribution so I will impute with the median as this is more robust to the outliers in the right tail\n","    - Family_Size also looks to follow a gamma distribution so I will impute with the median for the same reasons as above"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:09.639132Z","iopub.status.busy":"2024-07-31T15:41:09.638688Z","iopub.status.idle":"2024-07-31T15:41:09.669352Z","shell.execute_reply":"2024-07-31T15:41:09.667894Z","shell.execute_reply.started":"2024-07-31T15:41:09.639097Z"},"trusted":true},"outputs":[],"source":["train_data.fillna({'Ever_Married':'Missing'}, inplace=True)\n","\n","train_data.fillna({'Graduated': 'Missing'}, inplace=True)\n","\n","train_data.fillna({'Profession': 'Missing'}, inplace=True)\n","\n","train_data.fillna({'Var_1': 'Cat_6'}, inplace=True)\n","\n","train_data.fillna({'Work_Experience': train_data['Work_Experience'].median()}, inplace=True)\n","\n","train_data.fillna({'Family_Size': train_data['Family_Size'].median()}, inplace=True)\n","\n","new_missing_percentages = train_data.isnull().sum() * 100 / len(train_data)\n","print(new_missing_percentages)"]},{"cell_type":"markdown","metadata":{},"source":["## Scale all numeric features to [0,1] interval with min-max scaling. Why utilizing distance metrics in algorithms, it is important to use the same scale if you would like numeric featureas to be treated equally. Otherwise, the larger scaled features will be emphasized over the small scale features\n","\n","## Also drop ID column since this will be of no use in prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:11.578858Z","iopub.status.busy":"2024-07-31T15:41:11.578456Z","iopub.status.idle":"2024-07-31T15:41:11.609864Z","shell.execute_reply":"2024-07-31T15:41:11.608729Z","shell.execute_reply.started":"2024-07-31T15:41:11.578828Z"},"trusted":true},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","train_data.drop(columns=[\"ID\"], inplace=True)\n","\n","numeric_columns = train_data.select_dtypes(include=['int64', 'float64']).columns\n","\n","mm_scaler = MinMaxScaler()\n","train_data[numeric_columns] = mm_scaler.fit_transform(train_data[numeric_columns])\n","\n","print(train_data[numeric_columns].describe(include=\"all\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Now convert Categorical Variables to One-hot Encodings\n","\n","## First drop Segmentation Column! It is simply there for validation and not to be used for training since this is an unsupervised learning task!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:14.086333Z","iopub.status.busy":"2024-07-31T15:41:14.085880Z","iopub.status.idle":"2024-07-31T15:41:14.122423Z","shell.execute_reply":"2024-07-31T15:41:14.121232Z","shell.execute_reply.started":"2024-07-31T15:41:14.086300Z"},"trusted":true},"outputs":[],"source":["segmentation = train_data[\"Segmentation\"]\n","\n","train_data.drop(columns=[\"Segmentation\"], inplace=True)\n","\n","categorical_columns = train_data.select_dtypes(include=['object']).columns\n","\n","train_data = pd.get_dummies(train_data, columns=categorical_columns)\n","\n","print(train_data.head(5))"]},{"cell_type":"markdown","metadata":{},"source":["## Correlation Matrix on Transformed Data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:17.061672Z","iopub.status.busy":"2024-07-31T15:41:17.061260Z","iopub.status.idle":"2024-07-31T15:41:20.185599Z","shell.execute_reply":"2024-07-31T15:41:20.184157Z","shell.execute_reply.started":"2024-07-31T15:41:17.061641Z"},"trusted":true},"outputs":[],"source":["correlation = train_data.corr()\n","\n","plt.figure(figsize=(20, 20))\n","sns.heatmap(correlation, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n","plt.title('Correlation Matrix')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## The one-hot encoded columns will be highly correlated, especially when the possible values for that category is low. For example, the correlation between Graduated_Yes and Graduated_No is -.98. Intuitively this makes sense because knowing the value for one hot-encoded column allows us to determine the other. There is no perfect correlation because some values were missing for the data is not binary"]},{"cell_type":"markdown","metadata":{},"source":["## First try projecting down to 3 dimensions for visual purposes using:\n","    - PCA\n","    - t-SNE\n","    - Matrix Factorization(NMF)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:27.997626Z","iopub.status.busy":"2024-07-31T15:41:27.997211Z","iopub.status.idle":"2024-07-31T15:41:28.006341Z","shell.execute_reply":"2024-07-31T15:41:28.005004Z","shell.execute_reply.started":"2024-07-31T15:41:27.997594Z"},"trusted":true},"outputs":[],"source":["from sklearn.decomposition import PCA,TruncatedSVD\n","from sklearn.manifold import TSNE\n","\n","\n","unique_clusters = segmentation.unique()\n","colors = sns.color_palette('viridis', len(unique_clusters))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:31.190672Z","iopub.status.busy":"2024-07-31T15:41:31.190186Z","iopub.status.idle":"2024-07-31T15:41:32.029390Z","shell.execute_reply":"2024-07-31T15:41:32.028287Z","shell.execute_reply.started":"2024-07-31T15:41:31.190639Z"},"trusted":true},"outputs":[],"source":["pca = PCA(n_components=3, random_state=11)\n","pca_fit = pca.fit_transform(train_data.values)\n","\n","pca_df = pd.DataFrame({\"pca_one\": pca_fit[:,0], \"pca_two\": pca_fit[:,1], \"pca_three\": pca_fit[:,2], \"Segmentation\": segmentation})\n","\n","fig = plt.figure(figsize=(18, 10))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","\n","for segment, color in zip(unique_clusters, colors):\n","    subset = pca_df[pca_df['Segmentation'] == segment]\n","    ax.scatter(subset['pca_one'], subset['pca_two'], subset['pca_three'], label=segment, color=color)\n","    \n","ax.set_xlabel('pca_one')\n","ax.set_ylabel('pca_two')\n","ax.set_zlabel('pca_three')\n","ax.set_title('3D Scatter Plot of PCA Projection')\n","\n","ax.legend(title='Segment')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:41:45.440065Z","iopub.status.busy":"2024-07-31T15:41:45.439276Z","iopub.status.idle":"2024-07-31T15:43:21.998068Z","shell.execute_reply":"2024-07-31T15:43:21.996747Z","shell.execute_reply.started":"2024-07-31T15:41:45.440023Z"},"trusted":true},"outputs":[],"source":["tsne = TSNE(n_components=3, random_state=11)\n","tsne_fit = tsne.fit_transform(train_data)\n","\n","tsne_df = pd.DataFrame({\"tsne_one\": tsne_fit[:,0], \"tsne_two\": tsne_fit[:,1], \"tsne_three\": tsne_fit[:,2], \"Segmentation\": segmentation})\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","for segment, color in zip(unique_clusters, colors):\n","    subset = tsne_df[tsne_df['Segmentation'] == segment]\n","    ax.scatter(subset['tsne_one'], subset['tsne_two'], subset['tsne_three'], label=segment, color=color)\n","    \n","ax.set_xlabel('tsne_one')\n","ax.set_ylabel('tsne_two')\n","ax.set_zlabel('tsne_3')\n","ax.set_title('3D Scatter Plot of T-Sne Projection')\n","\n","ax.legend(title='Segment')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:44:11.153742Z","iopub.status.busy":"2024-07-31T15:44:11.153319Z","iopub.status.idle":"2024-07-31T15:44:12.032398Z","shell.execute_reply":"2024-07-31T15:44:12.031051Z","shell.execute_reply.started":"2024-07-31T15:44:11.153710Z"},"trusted":true},"outputs":[],"source":["svd = TruncatedSVD(n_components=3, random_state=11)\n","svd_fit = svd.fit_transform(train_data)\n","\n","svd_df = pd.DataFrame({\"svd_one\": svd_fit[:,0], \"svd_two\": svd_fit[:,1], \"svd_three\": svd_fit[:,2], \"Segmentation\": segmentation})\n","\n","fig = plt.figure(figsize=(16, 10))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","for segment, color in zip(unique_clusters, colors):\n","    subset = svd_df[tsne_df['Segmentation'] == segment]\n","    ax.scatter(subset['svd_one'], subset['svd_two'], subset['svd_three'], label=segment, color=color)\n","    \n","ax.set_xlabel('svd_one')\n","ax.set_ylabel('svd_two')\n","ax.set_zlabel('svd_3')\n","ax.set_title('3D Scatter Plot of SVD Projection')\n","\n","ax.legend(title='Segment')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:44:28.178016Z","iopub.status.busy":"2024-07-31T15:44:28.177559Z","iopub.status.idle":"2024-07-31T15:44:28.186610Z","shell.execute_reply":"2024-07-31T15:44:28.185433Z","shell.execute_reply.started":"2024-07-31T15:44:28.177974Z"},"trusted":true},"outputs":[],"source":["from scipy.optimize import linear_sum_assignment\n","\n","def create_label_mapping(y_true, y_pred):\n","\n","    cost_matrix = np.zeros((len(np.unique(y_true)), len(np.unique(y_pred))))\n","    for i, true_label in enumerate(np.unique(y_true)):\n","        for j, pred_label in enumerate(np.unique(y_pred)):\n","            cost_matrix[i, j] = -np.sum((y_true == true_label) & (y_pred == pred_label))\n","    \n","    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n","    \n","    label_mapping = {pred_label: true_label for pred_label, true_label in zip(col_ind, np.unique(y_true))}\n","    return label_mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:44:30.528397Z","iopub.status.busy":"2024-07-31T15:44:30.527982Z","iopub.status.idle":"2024-07-31T15:44:30.539201Z","shell.execute_reply":"2024-07-31T15:44:30.537790Z","shell.execute_reply.started":"2024-07-31T15:44:30.528366Z"},"trusted":true},"outputs":[],"source":["from sklearn.cluster import KMeans\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","pca_df.drop(columns=[\"Segmentation\"], inplace=True)\n","tsne_df.drop(columns=[\"Segmentation\"], inplace=True)\n","svd_df.drop(columns=[\"Segmentation\"], inplace=True)\n","\n","dataframes = {\"Original Transformed Data\": train_data, \"PCA to 3-D\": pca_df, \"T-Sne to 3-D\": tsne_df, \"SVD to 3-D\": svd_df}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-07-31T15:44:33.995852Z","iopub.status.busy":"2024-07-31T15:44:33.994836Z","iopub.status.idle":"2024-07-31T15:44:39.623804Z","shell.execute_reply":"2024-07-31T15:44:39.622703Z","shell.execute_reply.started":"2024-07-31T15:44:33.995815Z"},"trusted":true},"outputs":[],"source":["for key, df in dataframes.items():\n","    kmeans = KMeans(n_clusters=4, random_state=11, n_init=10)\n","    kmeans.fit(df)\n","    \n","    label_map = create_label_mapping(segmentation, kmeans.labels_)\n","    int_to_label = pd.Series(kmeans.labels_).map(label_map)\n","\n","    accuracy = accuracy_score(segmentation, int_to_label)\n","    \n","    print(f\"{key} Results:\\nInertia: {kmeans.inertia_}\\nDomain Specific Assignment Accuracy: {accuracy}\\n\\n\")\n","    \n","    cm = confusion_matrix(segmentation, int_to_label)\n","\n","    plt.figure(figsize=(10, 7))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","    plt.xlabel('Predicted Labels')\n","    plt.ylabel('True Labels')\n","    plt.title(f'{key} Confusion Matrix')\n","    plt.show()\n","    \n","    print(\"\\n\\n\\n\\n\")\n","    "]},{"cell_type":"markdown","metadata":{},"source":["## Interpreting Results:\n","    - For each kmeans model, I returned the inertia, assingment accuracy, and confusion matrix.\n","    \n","    - The inertia is the sum of squared distance to cluster centers, which is the optimization function of the k-means algorithm. The best inertia comes from the SVD model. \n","    \n","    -Note that the kmeans model trained on the high dim. dataset has a huge inertia relative to the other models. This is because the data space is so much higher so that is not a fair comparison. I trained to model to iterate this point and to compare assignment accuracies.\n","    \n","    - All models have awful accuracy scores of < 40%. This is because k-means is an unsupervised algorithm! The domain specific clustering information was not used in training the kmeans model. Therefore, none of the fitted kmeans models provide a source of validation for the clients hypothesis.\n","    \n","    - Just because none of the clustering in these different vector spaces confirms the hypothesis does not mean the hypothesis is wrong. It does I cannot further validate the customer's claim at this moment.\n","    \n","    - This is why it is so important to specify what the real objective is going into a ML problem. The client provided \"customer types\" that they found to be ground truth. \n","    \n","    - Suppose the problem is to verify that there exists a vector space where these assignments are validated, then dimensionality reduction and clustering are the algorithms to use.\n","    \n","    - If the problem is to accept the client's \"customer type\" clustering and to provide predictive capabilities for future customers, then a supervised algorithm would be the best course of action.\n","    \n","    - This notebook is indicated to showcase data cleaning and the implications from such cleaning operations, the reason for training a kmeans model, the limitations of such a model, and the importance of a clear objective in machine learning."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":848479,"sourceId":1447480,"sourceType":"datasetVersion"}],"dockerImageVersionId":30746,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
